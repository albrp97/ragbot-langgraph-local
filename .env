# =========================
# Core model + embeddings
# =========================
# Local chat model to generate answers.
MODEL_ID=Qwen/Qwen3-0.6B

# Text embedding model used for vector search.
EMBEDDING_ID=Qwen/Qwen3-Embedding-0.6B

# Where Chroma will persist the vector store.
CHROMA_PATH=./data/chroma


# =========================
# Memory / summarization
# =========================
# Max tokens to keep in conversation history before summarizing.
TOKEN_LIMIT_HISTORY=3000
# Target token budget for the summary when trimming history.
TOKEN_LIMIT_SUMMARY=1000


# =========================
# Runtime
# =========================
# Device selection: cuda | cpu | auto
DEVICE=cuda
# Gradio UI port.
GRADIO_PORT=7860


# =========================
# LLM generation defaults
# (can be overridden per call)
# =========================
# Sampling temperature (higher = more creative).
LLM_TEMPERATURE=0.7
# Nucleus sampling cap.
LLM_TOP_P=0.9
# Top-k sampling (0 disables top-k).
LLM_TOP_K=0
# Whether to sample (true) or use greedy decoding (false).
LLM_DO_SAMPLE=true
# Discourage repetition slightly for cleaner answers.
LLM_REPETITION_PENALTY=1.05
# Max new tokens generated per answer.
LLM_MAX_NEW_TOKENS=1024
# Seed for reproducibility (>=0 to activate).
LLM_SEED=0

# Toggle Qwen3 "thinking"/reasoning blocks. Keep false for a clean demo.
LLM_REASONING=false


# =========================
# Ingestion / Embeddings
# =========================
# Batch size for embedding computation (lower if RAM/VRAM spikes).
EMBEDDING_BATCH_SIZE=8
# Text chunk size (characters) for PDF splitting.
CHUNK_SIZE=1000
# Overlap between chunks (characters) to preserve context across splits.
CHUNK_OVERLAP=150


# =========================
# Retrieval (MMR)
# =========================
# Final number of contexts retrieved per query.
RETRIEVER_K=5


# =========================
# Image Retrieval
# =========================
IMAGE_RETRIEVAL_ID=Salesforce/blip-image-captioning-large
IMAGES_DIR=./data/processed/images

# =========================
# Object Detection
# =========================

DETECTOR_MODEL=yolov11n.pt
DETECTOR_PORT=8000